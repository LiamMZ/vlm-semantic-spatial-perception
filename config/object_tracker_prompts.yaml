# Prompts Configuration for VLM Object Tracking
# All prompts used by the ObjectTracker class

# Object Detection Prompts
detection:
  # Streaming detection prompt (preferred method)
  streaming:
    prior: |-
      You are a robotic vision system.  CONTEXT TURN ONLY (do NOT run detection yet).
      Use this turn to memorize prior IDs and reference images for re-identification when the current frame arrives next.
      Images in this turn are ONLY for context; the next turn will include the new frame for detection.

      Previously detected objects (reuse these EXACT ids if visible later; omit them if occluded/out of frame):
      {existing_objects_section}

      Prior observations (attached images listed below for re-identification; use them to match ids before creating new ones):
      [
      image: image content (see attached), id
      objects within: list of object label, position within the image
      ]
      {prior_images_section}

      IMPORTANT: All object IDs use underscores (_) to separate words, NOT hyphens (-). Examples: "blue_water_bottle" not "blue-water-bottle".

      Wait for the next turn with the current frame before performing detection.

    current: |-
      Detect all objects in this image. Return JSON array only:
      [{{"box_2d": [ymin, xmin, ymax, xmax], "label": "descriptive_name"}}]

      Reuse these IDs if visible: {existing_objects_section}
      Otherwise use descriptive names with underscores: blue_bottle, wooden_table, metal_spoon

      Detect both objects AND surfaces (tables, counters, floors).
      Box format: [ymin, xmin, ymax, xmax] as integers 0-1000.

# Object Analysis Prompts
analysis:
  response_schema:
    type: object
    required: ["object_type", "position"]
    properties:
      object_type:
        type: string
        description: Canonical object type or label.
      position:
        type: array
        description: Object center [y, x] normalized to 0-1000.
        minItems: 2
        maxItems: 2
        items:
          type: number
      affordances:
        type: array
        description: List of affordances with optional interaction points.
        items:
          type: object
          required: ["affordance"]
          properties:
            affordance:
              type: string
              description: Affordance name (e.g., graspable, pourable).
            reasoning:
              type: string
              description: Brief rationale for the interaction point.
            position:
              type: array
              description: Interaction point [y, x] normalized to 0-1000 (if applicable).
              minItems: 2
              maxItems: 2
              items:
                type: number
      predicates:
        type: array
        description: PDDL predicate strings using detected object ids (e.g., \"on cup_1 table_1\").
        items:
          type: string

  # Fast mode: only affordances and properties, no interaction points
  fast_mode: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}{task_context_section}

    Provide:
    1. Object position: Center point [y, x] in 0-1000 normalized coordinates
    2. Affordances: What robot actions are possible (include interaction point y/x for each when relevant)?
       - Common: graspable, pourable, containable, pushable, pullable, openable, closable, supportable, stackable
    3. PDDL Predicates (if requested):{predicates_section}

    Return JSON only. Follow this schema (also enforced via response_json_schema):
    {analysis_schema}

    All positions in 0-1000 normalized coords relative to THIS image.

  # Cached mode: use cached affordances, only detect interaction points
  cached_mode: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}
    
    Known affordances: {cached_affordances}
    
    For EACH affordance, identify the optimal interaction point and encode it inside the affordances list entries. Return JSON that follows this schema (also enforced via response_json_schema) but DO NOT add any new predicates:
    {analysis_schema}
    
    All positions in 0-1000 normalized coords relative to THIS image.

  # Full analysis mode
  full: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}{task_context_section}

    Provide detailed information:

    1. Object position: Center point [y, x] in 0-1000 normalized coordinates (relative to this image)
    2. Affordances: What robot actions are possible with this object? Include interaction point y/x for each affordance.
       - Common affordances: graspable, pourable, containable, pushable, pullable, openable, closable, supportable, stackable
    3. PDDL Predicates (if requested){predicates_section}

    Return JSON only. Follow this schema (also enforced via response_json_schema):
    {analysis_schema}
    Include `predicates` only when requested: {predicates_example}

    Note: All positions are in 0-1000 normalized coordinates relative to THIS image.
    Analyze the {object_name} based on its visual appearance.

# Interaction Point Update Prompt
interaction:
  update: |-
    You are a robotic manipulation system. Identify the optimal interaction point for {affordance} action on the {object_id}.
    
    Object type: {object_type}
    Action: {affordance}{task_context_section}
    
    Analyze the {object_id} and determine the best point to {affordance} based on:
    - Object shape and features
    - Intended action
    - Task requirements (if provided)
    
    Return in JSON format:
    {{
      "position": [y, x],
      "confidence": 0.0-1.0,
      "reasoning": "Detailed explanation"
    }}
    
    Position is [y, x] in 0-1000 normalized coordinates.

# Predicates Section Template
predicates:
  section_template: |

    5. PDDL Predicates: Identify all predicates that apply to this object.
       - Object ID for predicates: {object_id}
       - Available predicates (from PDDL domain): {pddl_list}
       - ONLY use predicates from the list above

       Detected objects in the scene (ALL object names MUST come from this list):{other_objects_list}

       **CRITICAL RULES FOR PREDICATES:**
       - ONLY use predicates from the available predicates list above
       - For unary predicates: "predicate_name {object_id}"
       - For binary/relational predicates: "predicate_name {object_id} other_object_id"
       - Use the EXACT {object_id} provided (not a variation or abbreviation)
       - Use the EXACT other_object_id provided in the detected scene objects list (not a variation or abbreviation)

       **CRITICAL: Object Name Validation**
       - EVERY object name in predicates MUST be from the detected objects list above
       - If analyzing a predicate like "on {object_id} counter", check that "counter" exists in detected objects
       - If analyzing a predicate like "in clamp {object_id}", check that "clamp" exists in detected objects
       - DO NOT use generic names like "table", "surface", "container" unless they appear in the detected objects list
       - DO NOT invent, abbreviate, or modify object names
       - If a relationship exists but the other object is not in the detected objects list, SKIP that predicate

       **Examples:**
       - ✓ CORRECT: "on blue_cup wooden_table" (if both blue_cup and wooden_table are in detected objects)
       - ✗ WRONG: "on blue_cup table" (if only "wooden_table" is detected, not "table")
       - ✗ WRONG: "in item container" (if "container" is not in detected objects list)
       - ✓ CORRECT: Skip the predicate if the other object is not detected

       Return a list of predicate strings that apply to this object.

  example_template: |
    ,
      "predicates": [
        "graspable {object_id}",
        "closed {object_id}",
        "on {object_id} table"
      ]

# Task Context Templates
task_context:
  # Detection context section
  detection_section_template: |

    Current Task: {task_description}

    Prioritize detecting objects relevant to this task.

  # Detection priority note
  detection_priority_template: |

    - PRIORITY: Focus especially on objects needed for: {task_description}

  # Analysis context section
  analysis_section_template: |

    Task Context: {task_description}
    Available Actions: {actions_list}

    Consider what affordances and interaction points would be most useful for this task.

  # Interaction point context section
  interaction_context_template: |

    Task Context: {task_description}
    The robot needs to perform: {action_description}

    Consider the task requirements when identifying the optimal interaction point.
