# Prompts Configuration for VLM Object Tracking
# All prompts used by the ObjectTracker class

# Object Detection Prompts
detection:
  # Streaming detection prompt (preferred method)
  streaming:
    prior: |-
      You are a robotic vision system.  CONTEXT TURN ONLY (do NOT run detection yet).
      Use this turn to memorize prior IDs and reference images for re-identification when the current frame arrives next.
      Images in this turn are ONLY for context; the next turn will include the new frame for detection.

      Previously detected objects (reuse these EXACT ids if visible later; omit them if occluded/out of frame):
      {existing_objects_section}

      Prior observations (attached images listed below for re-identification; use them to match ids before creating new ones):
      [
      image: image content (see attached), id
      objects within: list of object label, position within the image
      ]
      {prior_images_section}

      IMPORTANT: All object IDs use underscores (_) to separate words, NOT hyphens (-). Examples: "blue_water_bottle" not "blue-water-bottle".

      Wait for the next turn with the current frame before performing detection.

    current: |-
      You are a robotic vision system. CURRENT FRAME TURN: perform detection ONLY on the newly attached image below.
      Image order for this turn: the final attached image is the current frame for detection. Ignore any earlier turns and run detection ONLY on that last image.

      Previously detected objects (reuse these EXACT ids if visible; omit them if occluded/out of frame):
      {existing_objects_section}

      Instructions:
      - Return bounding boxes as a JSON array with labels. Never return masks or code fencing.
      - Include all objects you can identify, focusing on new objects that are not in context turn.
      - IMPORTANT: Detect BOTH manipulable objects AND surfaces/support structures:
        * Manipulable objects: bottles, cups, tools, containers, etc.
        * Surfaces: tables, counters, shelves, trays, workbenches, floors, etc.
        * Surfaces are critical for spatial predicates like "on" and "in"
      - If the current image is similar or identical to context image with no new objects, it is fine to not return new detections
      - If an object from previous images is visible, re-use the exact id as the "label"; otherwise create a descriptive, unique label (colors, size, position, distinctive traits).
      - IMPORTANT: Object labels must use underscores (_) to separate words, NOT hyphens (-). Examples: "blue_water_bottle" not "blue-water-bottle", "red_cup" not "red-cup", "wooden_table" not "wooden-table"
      - Box format: [ymin, xmin, ymax, xmax] normalized to 0-1000. All values MUST be integers.
      - Prefer tight boxes around each object instance. The bounding box may have moved compared to last capture, so make sure to update all bounding boxes and not just return the same bounding box for existing objects.
      - Output ONLY the JSON array in this format (no prose):
      [
        {{"box_2d": [ymin, xmin, ymax, xmax], "label": "<object label>"}}
      ]

# Object Analysis Prompts
analysis:
  # Fast mode: only affordances and properties, no interaction points
  fast_mode: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}{task_context_section}

    Provide:
    1. Object position: Center point [y, x] in 0-1000 normalized coordinates
    2. Affordances: What robot actions are possible?
       - Common: graspable, pourable, containable, pushable, pullable, openable, closable, supportable, stackable
    3. Properties: Color, size, material, state{predicates_section}

    Return JSON:
    {{
      "object_type": "cup",
      "position": [450, 320],
      "affordances": ["graspable", "pourable", "containable"],
      "properties": {{"color": "red", "material": "ceramic", "size": "medium", "state": "upright"}}{predicates_example},
      "confidence": 0.95
    }}

    All positions in 0-1000 normalized coords relative to THIS image.

  # Cached mode: use cached affordances, only detect interaction points
  cached_mode: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}
    
    Known affordances: {cached_affordances}
    
    For EACH affordance, identify the optimal interaction point:
    
    Return JSON:
    {{
      "object_type": "{object_type}",
      "position": [450, 320],
      "interaction_points": {{
        "graspable": {{"position": [450, 340], "confidence": 0.95, "reasoning": "..."}},
        "pourable": {{"position": [450, 320], "confidence": 0.90, "reasoning": "..."}}
      }},
      "confidence": 0.95
    }}
    
    All positions in 0-1000 normalized coords relative to THIS image.

  # Full analysis mode
  full: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}{task_context_section}

    Provide detailed information:

    1. Object position: Center point [y, x] in 0-1000 normalized coordinates (relative to this image)
    2. Affordances: What robot actions are possible with this object?
       - Common affordances: graspable, pourable, containable, pushable, pullable, openable, closable, supportable, stackable
    3. Interaction points: For EACH affordance, identify the optimal interaction point (relative to this image)
    4. Properties: Color, size, material, state, etc.{predicates_section}

    Return in JSON format:
    {{
      "object_type": "cup",
      "position": [450, 320],
      "affordances": ["graspable", "pourable", "containable"],
      "interaction_points": {{
        "graspable": {{
          "position": [450, 340],
          "confidence": 0.95,
          "reasoning": "Handle on the right side provides stable grasp point"
        }},
        "pourable": {{
          "position": [450, 320],
          "confidence": 0.90,
          "reasoning": "Top rim center for tilting and pouring"
        }}
      }},
      "properties": {{
        "color": "red",
        "material": "ceramic",
        "size": "medium",
        "state": "upright"
      }}{predicates_example},
      "confidence": 0.95
    }}

    Note: All positions are in 0-1000 normalized coordinates relative to THIS image.
    Analyze the {object_name} based on its visual appearance.

# Interaction Point Update Prompt
interaction:
  update: |-
    You are a robotic manipulation system. Identify the optimal interaction point for {affordance} action on the {object_id}.
    
    Object type: {object_type}
    Action: {affordance}{task_context_section}
    
    Analyze the {object_id} and determine the best point to {affordance} based on:
    - Object shape and features
    - Intended action
    - Task requirements (if provided)
    
    Return in JSON format:
    {{
      "position": [y, x],
      "confidence": 0.0-1.0,
      "reasoning": "Detailed explanation"
    }}
    
    Position is [y, x] in 0-1000 normalized coordinates.

# Predicates Section Template
predicates:
  section_template: |

    5. PDDL Predicates: Identify all predicates that apply to this object.
       - Object ID for predicates: {object_id}
       - Available predicates (from PDDL domain): {pddl_list}
       - ONLY use predicates from the list above

       Other detected objects in the scene (for relational predicates):{other_objects_list}

       For each applicable predicate from the list:
       - Use format: "predicate_name {object_id}" for unary predicates
       - Use format: "predicate_name {object_id} other_object" for binary predicates
       - Use exact object_id provided above
       - For relationships (on, in, near, etc.), the other_object MUST be from the detected objects list above
       - CRITICAL: Do NOT invent object names - only use object IDs from the detected objects list
       - If you need to reference a surface but it's not in the list, DO NOT add the predicate

       Return a list of predicate strings that apply to this object.

  example_template: |
    ,
      "predicates": [
        "graspable {object_id}",
        "closed {object_id}",
        "on {object_id} table"
      ]

# Task Context Templates
task_context:
  # Detection context section
  detection_section_template: |

    Current Task: {task_description}

    Prioritize detecting objects relevant to this task.

  # Detection priority note
  detection_priority_template: |

    - PRIORITY: Focus especially on objects needed for: {task_description}

  # Analysis context section
  analysis_section_template: |

    Task Context: {task_description}
    Available Actions: {actions_list}

    Consider what affordances and interaction points would be most useful for this task.

  # Interaction point context section
  interaction_context_template: |

    Task Context: {task_description}
    The robot needs to perform: {action_description}

    Consider the task requirements when identifying the optimal interaction point.

