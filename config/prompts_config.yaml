# Prompts Configuration for VLM Object Tracking
# All prompts used by the ObjectTracker class

# Object Detection Prompts
detection:
  # Streaming detection prompt (preferred method)
  streaming:
    prior: |-
      You are a robotic vision system.  CONTEXT TURN ONLY (do NOT run detection yet).
      Use this turn to memorize prior IDs and reference images for re-identification when the current frame arrives next.
      Images in this turn are ONLY for context; the next turn will include the new frame for detection.

      Previously detected objects (reuse these EXACT ids if visible later; omit them if occluded/out of frame):
      {existing_objects_section}

      Prior observations (attached images listed below for re-identification; use them to match ids before creating new ones):
      [
      image: image content (see attached), id
      objects within: list of object label, position within the image
      ]
      {prior_images_section}

      Wait for the next turn with the current frame before performing detection.

    current: |-
      You are a robotic vision system. CURRENT FRAME TURN: perform detection ONLY on the newly attached image below.
      Image order for this turn: the final attached image is the current frame for detection. Ignore any earlier turns and run detection ONLY on that last image.

      Previously detected objects (reuse these EXACT ids if visible; omit them if occluded/out of frame):
      {existing_objects_section}

      Instructions:
      - Return bounding boxes as a JSON array with labels. Never return masks or code fencing.
      - Include all objects you can identify, focusing on new objects that are not in context turn. 
      - If the current image is similar or identical to context image with no new objects, it is fine to not return new detections
      - If an object from previous images is visible, re-use the exact id as the "label"; otherwise create a descriptive, unique label (colors, size, position, distinctive traits).
      - Box format: [ymin, xmin, ymax, xmax] normalized to 0-1000. All values MUST be integers.
      - Prefer tight boxes around each object instance. The bounding box may have moved compared to last capture, so make sure to update all bounding boxes and not just return the same bounding box for existing objects.
      - Output ONLY the JSON array in this format (no prose):
      [
        {{"box_2d": [ymin, xmin, ymax, xmax], "label": "<object label>"}}
      ]

# Object Analysis Prompts
analysis:
  # Fast mode: only affordances and properties, no interaction points
  fast_mode: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}{task_context_section}

    Provide:
    1. Object position: Center point [y, x] in 0-1000 normalized coordinates
    2. Affordances: What robot actions are possible?
       - Common: graspable, pourable, containable, pushable, pullable, openable, closable, supportable, stackable
    3. Properties: Color, size, material, state{pddl_section}

    Return JSON:
    {{
      "object_type": "cup",
      "position": [450, 320],
      "affordances": ["graspable", "pourable", "containable"],
      "properties": {{"color": "red", "material": "ceramic", "size": "medium", "state": "upright"}}{pddl_example},
      "confidence": 0.95
    }}

    All positions in 0-1000 normalized coords relative to THIS image.

  # Cached mode: use cached affordances, only detect interaction points
  cached_mode: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}
    
    Known affordances: {cached_affordances}
    
    For EACH affordance, identify the optimal interaction point:
    
    Return JSON:
    {{
      "object_type": "{object_type}",
      "position": [450, 320],
      "interaction_points": {{
        "graspable": {{"position": [450, 340], "confidence": 0.95, "reasoning": "..."}},
        "pourable": {{"position": [450, 320], "confidence": 0.90, "reasoning": "..."}}
      }},
      "confidence": 0.95
    }}
    
    All positions in 0-1000 normalized coords relative to THIS image.

  # Full analysis mode
  full: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}{task_context_section}

    Provide detailed information:

    1. Object position: Center point [y, x] in 0-1000 normalized coordinates (relative to this image)
    2. Affordances: What robot actions are possible with this object?
       - Common affordances: graspable, pourable, containable, pushable, pullable, openable, closable, supportable, stackable
    3. Interaction points: For EACH affordance, identify the optimal interaction point (relative to this image)
    4. Properties: Color, size, material, state, etc.{pddl_section}

    Return in JSON format:
    {{
      "object_type": "cup",
      "position": [450, 320],
      "affordances": ["graspable", "pourable", "containable"],
      "interaction_points": {{
        "graspable": {{
          "position": [450, 340],
          "confidence": 0.95,
          "reasoning": "Handle on the right side provides stable grasp point"
        }},
        "pourable": {{
          "position": [450, 320],
          "confidence": 0.90,
          "reasoning": "Top rim center for tilting and pouring"
        }}
      }},
      "properties": {{
        "color": "red",
        "material": "ceramic",
        "size": "medium",
        "state": "upright"
      }}{pddl_example},
      "confidence": 0.95
    }}

    Note: All positions are in 0-1000 normalized coordinates relative to THIS image.
    Analyze the {object_name} based on its visual appearance.

# Interaction Point Update Prompt
interaction:
  update: |-
    You are a robotic manipulation system. Identify the optimal interaction point for {affordance} action on the {object_id}.
    
    Object type: {object_type}
    Action: {affordance}{task_context_section}
    
    Analyze the {object_id} and determine the best point to {affordance} based on:
    - Object shape and features
    - Intended action
    - Task requirements (if provided)
    
    Return in JSON format:
    {{
      "position": [y, x],
      "confidence": 0.0-1.0,
      "reasoning": "Detailed explanation"
    }}
    
    Position is [y, x] in 0-1000 normalized coordinates.

# PDDL Predicate Section Template
pddl:
  section_template: |

    4. PDDL State Predicates: For each of these predicates, determine if it applies to this object:
       - Predicates to check: {pddl_list}
       - Return true/false for each predicate based on visual observation
       - Examples:
         * "clean" - object appears clean vs dirty
         * "opened" - container/door is open vs closed
         * "filled" - container has contents vs empty
         * "wet" - object appears wet vs dry

  example_template: |
    ,
      "pddl_state": {"clean": true, "opened": false, "filled": false}
<<<<<<< HEAD
=======

# Task Context Templates
task_context:
  # Detection context section
  detection_section_template: |

    Current Task: {task_description}

    Prioritize detecting objects relevant to this task.

  # Detection priority note
  detection_priority_template: |

    - PRIORITY: Focus especially on objects needed for: {task_description}

  # Analysis context section
  analysis_section_template: |

    Task Context: {task_description}
    Available Actions: {actions_list}

    Consider what affordances and interaction points would be most useful for this task.

  # Interaction point context section
  interaction_context_template: |

    Task Context: {task_description}
    The robot needs to perform: {action_description}

    Consider the task requirements when identifying the optimal interaction point.

>>>>>>> aa951db (pddl solver integration setup)
