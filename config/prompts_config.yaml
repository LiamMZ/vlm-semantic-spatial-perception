# Prompts Configuration for VLM Object Tracking
# All prompts used by the ObjectTracker class

# Object Detection Prompts
detection:
  # Streaming detection prompt (preferred method)
  streaming: |-
    You are a robotic vision system. Identify all distinct objects in this image.
    Attached media: the first image is the current frame; subsequent images (if any) are prior observations listed below.
    
    Previously detected objects (reuse these EXACT ids if visible; omit them if occluded/out of frame):
    {existing_objects_section}

    Prior observations (attached images listed below for re-identification; use them to match ids before creating new ones):
    [
    image: image content (see attached), id
    objects within: list of object label, position within the image
    ]
    {prior_images_section}
    
    For each object, provide a descriptive name and bounding box.
    
    Return object info ONE PER LINE in this format:
    OBJECT: red cup | [400, 280, 500, 360]
    OBJECT: blue bottle | [200, 450, 550, 620]
    OBJECT: white bowl | [150, 100, 300, 250]
    END
    
    Important:
    - If you see a listed object, reuse the same id as the object name with an updated bounding box
    - Only introduce a new id when no listed object clearly matches what you see
    - Never duplicate the same real object with multiple ids
    - Include color or distinguishing features in names
    - Be specific (e.g., "red cup" not just "cup")
    - Bounding box format: [y1, x1, y2, x2] in 0-1000 normalized coordinates
    - Each object on a new line: "OBJECT: name | [y1, x1, y2, x2]"
    - End with "END" on its own line
    - Focus on manipulatable objects and surfaces

  # Batch detection prompt (fallback)
  batch: |-
    You are a robotic vision system. Identify all distinct objects in this image.
    Attached media: the first image is the current frame; subsequent images (if any) are prior observations listed below.
    
    Previously detected objects (reuse these EXACT ids if visible; omit them if occluded/out of frame):
    {existing_objects_section}

    Prior observations (attached images listed below for re-identification; use them to match ids before creating new ones):
    [
    image: image content (see attached), id
    objects within: list of object label, position within the image
    ]
    {prior_images_section}
    
    For each object, provide a descriptive name and bounding box.
    
    Return ONLY a JSON array with object info:
    {{
      "objects": [
        {{"name": "red cup", "bbox": [400, 280, 500, 360]}},
        {{"name": "blue bottle", "bbox": [200, 450, 550, 620]}},
        {{"name": "white bowl", "bbox": [150, 100, 300, 250]}}
      ]
    }}
    
    Important:
    - If you see a listed object, reuse the same id as the object name with an updated bounding box
    - Only introduce a new id when no listed object clearly matches what you see
    - Never duplicate the same real object with multiple ids
    - Include color or distinguishing features in names
    - Be specific (e.g., "red cup" not just "cup")
    - Bounding box format: [y1, x1, y2, x2] in 0-1000 normalized coordinates
    - Focus on manipulatable objects and surfaces

# Object Analysis Prompts
analysis:
  # Fast mode: only affordances and properties, no interaction points
  fast_mode: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}
    
    Provide:
    1. Object position: Center point [y, x] in 0-1000 normalized coordinates
    2. Affordances: What robot actions are possible?
       - Common: graspable, pourable, containable, pushable, pullable, openable, closable, supportable, stackable
    3. Properties: Color, size, material, state{pddl_section}
    
    Return JSON:
    {{
      "object_type": "cup",
      "position": [450, 320],
      "affordances": ["graspable", "pourable", "containable"],
      "properties": {{"color": "red", "material": "ceramic", "size": "medium", "state": "upright"}}{pddl_example},
      "confidence": 0.95
    }}
    
    All positions in 0-1000 normalized coords relative to THIS image.

  # Cached mode: use cached affordances, only detect interaction points
  cached_mode: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}
    
    Known affordances: {cached_affordances}
    
    For EACH affordance, identify the optimal interaction point:
    
    Return JSON:
    {{
      "object_type": "{object_type}",
      "position": [450, 320],
      "interaction_points": {{
        "graspable": {{"position": [450, 340], "confidence": 0.95, "reasoning": "..."}},
        "pourable": {{"position": [450, 320], "confidence": 0.90, "reasoning": "..."}}
      }},
      "confidence": 0.95
    }}
    
    All positions in 0-1000 normalized coords relative to THIS image.

  # Full analysis mode
  full: |-
    You are a robotic manipulation system. Analyze the {object_name} in this image.{crop_note}
    
    Provide detailed information:
    
    1. Object position: Center point [y, x] in 0-1000 normalized coordinates (relative to this image)
    2. Affordances: What robot actions are possible with this object?
       - Common affordances: graspable, pourable, containable, pushable, pullable, openable, closable, supportable, stackable
    3. Interaction points: For EACH affordance, identify the optimal interaction point (relative to this image)
    4. Properties: Color, size, material, state, etc.{pddl_section}
    
    Return in JSON format:
    {{
      "object_type": "cup",
      "position": [450, 320],
      "affordances": ["graspable", "pourable", "containable"],
      "interaction_points": {{
        "graspable": {{
          "position": [450, 340],
          "confidence": 0.95,
          "reasoning": "Handle on the right side provides stable grasp point"
        }},
        "pourable": {{
          "position": [450, 320],
          "confidence": 0.90,
          "reasoning": "Top rim center for tilting and pouring"
        }}
      }},
      "properties": {{
        "color": "red",
        "material": "ceramic",
        "size": "medium",
        "state": "upright"
      }}{pddl_example},
      "confidence": 0.95
    }}
    
    Note: All positions are in 0-1000 normalized coordinates relative to THIS image.
    Analyze the {object_name} based on its visual appearance.

# Interaction Point Update Prompt
interaction:
  update: |-
    You are a robotic manipulation system. Identify the optimal interaction point for {affordance} action on the {object_id}.
    
    Object type: {object_type}
    Action: {affordance}{task_context_section}
    
    Analyze the {object_id} and determine the best point to {affordance} based on:
    - Object shape and features
    - Intended action
    - Task requirements (if provided)
    
    Return in JSON format:
    {{
      "position": [y, x],
      "confidence": 0.0-1.0,
      "reasoning": "Detailed explanation"
    }}
    
    Position is [y, x] in 0-1000 normalized coordinates.

# PDDL Predicate Section Template
pddl:
  section_template: |
    
    4. PDDL State Predicates: For each of these predicates, determine if it applies to this object:
       - Predicates to check: {pddl_list}
       - Return true/false for each predicate based on visual observation
       - Examples:
         * "clean" - object appears clean vs dirty
         * "opened" - container/door is open vs closed
         * "filled" - container has contents vs empty
         * "wet" - object appears wet vs dry

  example_template: |
    ,
      "pddl_state": {"clean": true, "opened": false, "filled": false}
