### 2025-12-02-1950 – Map Gemini action key into PDDL generation

- **Owner**: AI Agent (bug fix)
- **Agents affected**: LLMTaskAnalyzer, docs/planning
- **Git commit(s)**: Based on 283854f544275d883fb709f2bfd16bd65b900392
- **Context**: Replay runs produced domains with no actions even though Gemini returned `relevant_actions`; the parser only read `required_actions`, so the maintainer never added LLM-suggested action schemas.
- **Actions**:
  - Updated `LLMTaskAnalyzer._parse_response` to accept `required_actions`, `relevant_actions`, or `actions` keys when populating `TaskAnalysis`.
  - Documented the alias handling in `docs/planning.md` to keep the task-analysis section accurate.
- **Result**: Actions proposed by Gemini now flow into the PDDL domain during task analysis and show up in generated domain files.
- **References**: `src/planning/llm_task_analyzer.py`, `docs/planning.md`

### 2025-12-02-1909 – Snapshot-based demo replay utility

- **Owner**: AI Agent (simulation tooling)
- **Agents affected**: Orchestrator scripts, docs/planning, README
- **Git commit(s)**: Based on 283854f544275d883fb709f2bfd16bd65b900392
- **Context**: Needed a quick way to rerun the orchestrator/LLM pipeline against a finished demo (using its cached snapshots) without turning on the RealSense camera.
- **Actions**:
  - Added `scripts/replay_cached_demo.py`, including a lightweight `SnapshotCamera` that streams cached frames, waits for the desired detection count, and writes outputs + GenAI logs to a new timestamped directory.
  - Updated `README.md` (Running + directory overview) and `docs/planning.md` (useful commands) to document the new replay command.
- **Result**: Any past `outputs/demos/<run>` folder can be “simulated” again—task analysis, detection prompts, and state exports are regenerated while reusing the saved color/depth/ intrinsics frames.
- **References**: `scripts/replay_cached_demo.py`, `README.md`, `docs/planning.md`

### 2025-12-02-1857 – Strip predicate types and normalize arity

- **Owner**: AI Agent (planning maintenance)
- **Agents affected**: LLMTaskAnalyzer prompts, PDDLDomainMaintainer, docs/planning
- **Git commit(s)**: Based on 283854f544275d883fb709f2bfd16bd65b900392
- **Context**: Task analysis started emitting typed predicate signatures, which the maintainer stored verbatim; registry predicates (`is_bottle`, `on`, etc.) no longer matched, so `:init` stayed empty even when detections succeeded.
- **Actions**:
  - Updated `config/llm_task_analyzer_prompts.yaml` to explicitly forbid type annotations in `relevant_predicates`, keeping only positional variables when needed.
  - Added a predicate-signature normalizer in `pddl_domain_maintainer.py` that strips types, sanitizes names, infers arity, and rehydrates an untyped display string for the tracker.
  - Replaced the task-analysis predicate list with the normalized strings and taught `is_domain_complete()` to parse them back into canonical names before validating.
  - Documented the “no typed predicates” rule inside `docs/planning.md`.
- **Result**: LLM outputs now stay untyped, the maintainer registers multi-arity predicates with the correct argument count, and initial literals can once again be added to the problem file.
- **References**: `config/llm_task_analyzer_prompts.yaml`, `src/planning/pddl_domain_maintainer.py`, `docs/planning.md`

### 2025-12-02-1817 – GenAI log viewer built and iterated

- **Owner**: AI Agent (tooling)
- **Agents affected**: GenAI log viewer, GenAI logging utility, docs
- **Git commit(s)**: Based on 283854f544275d883fb709f2bfd16bd65b900392
- **Context**: Build a Textual UI for browsing GenAI request/response logs and polish it for readability, media handling, and invocation ergonomics.
- **Actions**:
  - Added `examples/genai_viewer.py` with sidebar navigation, per-call metadata/request/response panes, media toggle, and keyboard controls.
  - Iterated UI: caller on its own line with dividers, compact time/media/text counts, light mode default, resized panels with model/config in metadata, and optional media panel (`m` toggle).
  - Improved content rendering: walk nested `parts` (with roles), parse token usage from streams, read text from saved files, show media placeholders even when bytes are missing, and make paths relative to project root.
  - Updated CLI to require an explicit path and auto-resolve `genai_logs` when a parent world/output dir is provided; documented usage in README.
  - Enhanced logging utility to record inline_data metadata when payload bytes are absent so media hints surface in logs.
- **Result**: GenAI traces are browseable in-terminal with accurate prompts/responses and clearer media/context cues; invocation is explicit and works from either the log dir or its parent output directory.
- **References**: `examples/genai_viewer.py`, `src/utils/genai_logging.py`, `README.md`

### 2025-12-02-1721 – GenAI request logging and demo wiring

- **Owner**: AI Agent (logging instrumentation)
- **Agents affected**: GenAI client, orchestrator demo, docs
- **Git commit(s)**: Based on 283854f544275d883fb709f2bfd16bd65b900392
- **Context**: Add a universal override that captures every GenAI request/response (including media) and ensure the orchestrator demo writes those traces alongside its world outputs.
- **Actions**:
  - Introduced a GenAI logging utility that monkeypatches `genai.Client` to log sync and async/streaming `generate_content` calls into uniquely named folders with request payloads, responses, media blobs, and caller metadata.
  - Wired `orchestrator_demo` to enable the logger automatically and store artifacts under `<output>/genai_logs/`, surfacing the path in the TUI during initialization.
  - Documented the new GenAI logging output location in the orchestrator docs.
- **Result**: All GenAI traffic is now auditable without modifying individual clients, and demo runs persist per-call logs beside the world state outputs for inspection.
- **References**: `src/utils/genai_logging.py`, `examples/orchestrator_demo.py`, `docs/ORCHESTRATOR.md`

### 2025-12-02-1642 – Collapse task analyzer onto single analysis template

- **Owner**: AI Agent (prompt maintenance)
- **Agents affected**: LLMTaskAnalyzer, docs/planning
- **Git commit(s)**: Based on 8ebbf948e736f81fb58c6f3dd10e8e055835edf5
- **Context**: Align the task analyzer so every run—initial or observation-backed—renders the same analysis instructions and remove the redundant `initial_prompt` definition from the prompt pack.
- **Actions**:
  - Replaced the bespoke `initial_prompt` text with the analysis template rendered for zero observations, adding placeholder descriptions for missing objects/relationships.
  - Deleted `initial_prompt` from `config/llm_task_analyzer_prompts.yaml` to keep `analysis_prompt` as the sole authoritative template.
  - Updated `LLMTaskAnalyzer` to always render that template while injecting placeholder summaries whenever perception returns no objects or relationships.
  - Refreshed `docs/planning.md` to document the single-template behavior.
- **Result**: Initial and observation-backed analyses now share identical formatting, and the YAML no longer duplicates instructions while still informing the LLM when perception data is absent.
- **References**: `config/llm_task_analyzer_prompts.yaml`, `src/planning/llm_task_analyzer.py`, `docs/planning.md`

### 2025-12-02-1455 – Prompt config alignment across perception + planning

- **Owner**: AI Agent (config maintenance)
- **Agents affected**: ObjectTracker, LLMTaskAnalyzer, docs/perception, docs/planning, agent ops docs
- **Git commit(s)**: Based on ed3cdbf027bf1535738633be1e60567a21ea7d34
- **Context**: Align the perception prompt pack name with its scope (ObjectTracker-only), externalize the LLM task analyzer prompts into YAML like the rest of the stack, and prune unused config files.
- **Actions**:
  - Renamed `config/prompts_config.yaml` to `config/object_tracker_prompts.yaml` via `git mv`.
  - Updated `ObjectTracker.DEFAULT_PROMPTS_CONFIG` and related doc strings to point to the new path.
  - Added `config/llm_task_analyzer_prompts.yaml`, wired `LLMTaskAnalyzer` to load it (with override support), and introduced shared template rendering helpers.
  - Compactified the LLM task analyzer prompts so only dynamic values (task text, robot description, JSON summaries of objects/relationships) are templated; static instructions now live entirely in the YAML, which now owns the optional robot capability description (init arg removed).
  - Externalized the domain-refinement prompt to `config/pddl_domain_maintainer_prompts.yaml`, added a prompts loader to `PDDLDomainMaintainer`, and now render the refinement template with only dynamic sections (error text, goal objects, robot context, domain/problem snippets).
  - Deleted unused `config/gemini.yaml`, `config/gemini_config.yaml`, and `config/perception_config.yaml`.
  - Refreshed planning + agent documentation (`docs/planning.md`, `docs/README.md`, `agents/design/architecture.md`, `agents/operations/playbook.md`, `AGENTS.md`) to reference the new configs and upkeep rules.
- **Result**: Both perception and planning layers now load prompts from explicit YAML packs, and stale configs were removed to avoid drift.
- **References**: `config/object_tracker_prompts.yaml`, `config/llm_task_analyzer_prompts.yaml`, `config/pddl_domain_maintainer_prompts.yaml`, `src/perception/object_tracker.py`, `src/planning/llm_task_analyzer.py`, `src/planning/pddl_domain_maintainer.py`, `docs/perception.md`, `docs/planning.md`, `docs/README.md`, `agents/design/prompts_configuration.md`, `agents/design/architecture.md`, `agents/operations/playbook.md`, `AGENTS.md`
