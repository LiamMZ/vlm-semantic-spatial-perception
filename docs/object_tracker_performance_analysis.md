# Object Tracker Performance Analysis

## Executive Summary

The initial object detection is slow due to **unlimited concurrent VLM requests** and **per-object analysis overhead**. With 8 objects, the system fires 8+ concurrent VLM requests, each processing cropped images and analyzing affordances.

---

## Performance Bottlenecks

### ðŸ”´ CRITICAL: No Rate Limiting on VLM Requests

**Location:** [src/perception/object_tracker.py:530](../src/perception/object_tracker.py#L530)

```python
# Step 2: Analyze all objects concurrently using asyncio.gather
tasks = [
    self._analyze_single_object(...)
    for object_name, bounding_box in object_data_list
]

# Run all analysis tasks concurrently - NO LIMIT!
results = await asyncio.gather(*tasks, return_exceptions=True)
```

**Problem:**
- `max_parallel_requests = 5` is defined but **never enforced**
- All objects analyzed simultaneously â†’ burst of VLM API calls
- With 8 objects: 8 concurrent VLM requests
- Each request includes image encoding, prompt building, VLM inference

**Impact:**
- API rate limiting kicks in
- Requests queue up and slow down
- No benefit from parallelism due to rate limits
- Higher latency due to concurrent overhead

---

### ðŸŸ¡ MAJOR: Two-Phase Detection Process

**Phase 1: Object Name Detection** (line 490-498)
- Single VLM request to detect all object names and bounding boxes
- Streams results as they arrive
- **Time:** 2-10 seconds depending on scene complexity

**Phase 2: Per-Object Analysis** (line 516-530)
- Separate VLM request FOR EACH object
- Crops image, resizes to 512x512
- Analyzes affordances, properties, PDDL predicates
- **Time:** 3-8 seconds per object Ã— N objects

**Total Time = Phase 1 + (Phase 2 Ã— N objects)**

For 8 objects: 2-10s + (3-8s Ã— 8) = **26-74 seconds**

---

### ðŸŸ¡ MAJOR: Image Cropping and Resizing Overhead

**Location:** [src/perception/object_tracker.py:682-705](../src/perception/object_tracker.py#L682-L705)

```python
# Crop the image (PIL uses left, upper, right, lower)
analysis_image = image.crop((px1, py1, px2, py2))

# Resize crop to target size for faster VLM processing
if self.crop_target_size > 0:
    analysis_image = analysis_image.resize(
        (self.crop_target_size, self.crop_target_size),
        Image.Resampling.LANCZOS
    )
```

**Issue:**
- Crops and resizes **for every object**
- LANCZOS resampling is high-quality but slow
- Happens even when `fast_mode=True`

**Impact:**
- 100-300ms per object for image processing
- With 8 objects: 0.8-2.4s wasted on image ops

---

### ðŸŸ  MODERATE: PDDL Predicate Context Building

**Location:** [src/perception/object_tracker.py:722-734](../src/perception/object_tracker.py#L722-L734)

```python
# Get list of other detected objects for relational predicates
other_objects = self.registry.get_all_objects()
other_objects_list = ""
if other_objects:
    other_objects_list = "\n"
    for obj in other_objects:
        # Skip the current object being analyzed
        if obj.object_id != existing_object_id and obj.object_id != object_name:
            other_objects_list += f"       - {obj.object_id} ({obj.object_type})\n"
```

**Issue:**
- Queries registry for every object analysis
- Builds string list of other objects
- Happens even if predicates aren't needed

**Impact:**
- Minimal (<10ms) but unnecessary work
- Prompt becomes longer â†’ more VLM tokens

---

### ðŸŸ  MODERATE: Prior Observations for Re-ID

**Location:** [src/perception/object_tracker.py:447-459](../src/perception/object_tracker.py#L447-L459)

```python
# Legacy VLM-based re-identification
prior_observations = self._select_latest_reid_observations(
    existing_objects,
    list(self._recent_observations)
)
additional_image_parts = [
    types.Part.from_bytes(data=obs["image_bytes"], mime_type="image/png")
    for obs in prior_observations
    if obs.get("image_bytes")
]
```

**Issue (when spatial_tracking=False):**
- Loads previous frame images from memory
- Encodes them as image parts
- Sends multiple images to VLM for re-ID
- Much higher token cost

**Impact:**
- 2-5x more tokens per request
- Significantly slower initial detection
- **FIXED** when `enable_spatial_tracking=True` (now default)

---

## Timeline Breakdown

### Initial Detection (8 objects, typical case):

```
0.0s  - Start detection
0.1s  - Image encoding (1920x1080 â†’ PNG)
0.2s  - Build detection prompt
2.5s  - VLM: Detect object names & bounding boxes (streaming)
      â†’ Got 8 objects with bboxes

3.0s  - Start parallel analysis (8 tasks launched)
3.1s  - Crop & resize 8 images (concurrent, ~100ms each)
3.5s  - VLM request #1-5 start (rate limit = 5)
7.0s  - VLM request #1 completes
7.1s  - VLM request #6 starts
10.5s - VLM request #2 completes
10.6s - VLM request #7 starts
14.0s - VLM request #3 completes
14.1s - VLM request #8 starts
17.5s - VLM request #4 completes
21.0s - VLM request #5 completes
24.5s - VLM request #6 completes
28.0s - VLM request #7 completes
31.5s - VLM request #8 completes

31.6s - Spatial tracking (5ms)
31.7s - Registry updates (2ms)
31.8s - TOTAL TIME
```

**Expected: ~31-35 seconds for 8 objects**

---

## Recommended Fixes

### 1. âš¡ CRITICAL: Implement Semaphore for Rate Limiting

**Change:** [src/perception/object_tracker.py:513-530](../src/perception/object_tracker.py#L513-L530)

```python
# Add semaphore to constructor
def __init__(self, ...):
    ...
    self._analysis_semaphore = asyncio.Semaphore(self.max_parallel_requests)

# Use semaphore in detection
async def detect_objects(...):
    ...
    # Step 2: Analyze all objects with rate limiting
    async def analyze_with_limit(object_name, bounding_box):
        async with self._analysis_semaphore:
            return await self._analyze_single_object(
                object_name, pil_image, depth_frame,
                camera_intrinsics, temperature, bounding_box,
                existing_object_id=object_name if object_name in existing_ids else None
            )

    tasks = [
        analyze_with_limit(object_name, bounding_box)
        for object_name, bounding_box in object_data_list
    ]

    results = await asyncio.gather(*tasks, return_exceptions=True)
```

**Expected Improvement:**
- Controlled concurrency prevents API rate limiting
- More predictable performance
- **20-30% faster** by avoiding queue buildup

---

### 2. âš¡ HIGH: Batch Object Analysis

**Idea:** Send all objects to VLM in a single request instead of N separate requests

```python
# Instead of N requests:
for obj in objects:
    analyze_single_object(obj)  # 8 VLM calls

# Single batched request:
analyze_all_objects(objects)  # 1 VLM call
```

**Prompt:**
```
Analyze these 8 objects in the image:
1. blue_water_bottle (bbox: [100, 200, 300, 400])
2. orange_marker (bbox: [500, 100, 600, 200])
...

For each object, return:
- affordances
- object_type
- properties
- pddl_predicates (if applicable)
```

**Expected Improvement:**
- **5-10x faster** (single 10s request vs 8Ã—4s requests)
- Much lower token cost
- Simpler code

---

### 3. âš¡ MEDIUM: Use Bilinear Instead of LANCZOS

**Change:** [src/perception/object_tracker.py:702-705](../src/perception/object_tracker.py#L702-L705)

```python
# Before
analysis_image = analysis_image.resize(
    (self.crop_target_size, self.crop_target_size),
    Image.Resampling.LANCZOS  # High quality, slow
)

# After
analysis_image = analysis_image.resize(
    (self.crop_target_size, self.crop_target_size),
    Image.Resampling.BILINEAR  # Fast, good enough
)
```

**Expected Improvement:**
- **50-70% faster** image resizing
- Saves 0.5-1.5s total with 8 objects
- Minimal quality loss for VLM

---

### 4. âš¡ MEDIUM: Skip Cropping for Small Objects

**Idea:** Don't crop if bounding box is >30% of image

```python
if bounding_box is not None:
    bbox_area = (y2 - y1) * (x2 - x1) / (1000 * 1000)  # Normalized area
    if bbox_area < 0.3:  # Only crop if <30% of image
        # Crop and resize
        ...
    else:
        # Object is large, use full image
        analysis_image = image
```

**Expected Improvement:**
- Skips crop/resize for large objects
- Saves 100-300ms per large object

---

### 5. âš¡ LOW: Lazy PDDL Context Building

**Change:** [src/perception/object_tracker.py:719-743](../src/perception/object_tracker.py#L719-L743)

```python
# Only build if pddl_predicates is non-empty
if self.pddl_predicates:
    # Get other objects once, outside loop
    other_objects = self.registry.get_all_objects()
    ...
```

**Expected Improvement:**
- Minor (<10ms per object)
- Cleaner code

---

### 6. âš¡ LOW: Cache Image Encoding

Already implemented! ([line 1572-1578](../src/perception/object_tracker.py#L1572-L1578))

```python
# Check if we can use cached encoded image
if (self._encoded_image_cache is not None and
    self._cache_image_id == id(image)):
    img_bytes = self._encoded_image_cache
```

âœ… Good optimization, no changes needed.

---

## Priority Recommendations

### Immediate (Do First):
1. **Implement Semaphore** â†’ 20-30% faster, easy fix
2. **Use BILINEAR resize** â†’ 5-10% faster, one-line change
3. **Enable spatial tracking** â†’ Already done! Removes multi-image overhead

### High Value (Do Next):
4. **Batch object analysis** â†’ 5-10x faster, requires prompt redesign
5. **Skip crop for large objects** â†’ Saves 10-20% image processing time

### Nice to Have:
6. **Lazy PDDL context** â†’ Minor improvement, cleaner code

---

## Configuration Tuning

### Current Defaults:
```python
ObjectTracker(
    max_parallel_requests=5,       # âš ï¸ Not enforced!
    crop_target_size=512,           # OK
    enable_affordance_caching=True, # âœ… Good
    fast_mode=True,                 # âœ… Good
    enable_spatial_tracking=True    # âœ… Just added!
)
```

### Recommended Tuning:
```python
# For FASTEST detection (less accurate):
ObjectTracker(
    max_parallel_requests=3,        # Lower = more stable
    crop_target_size=256,           # Smaller = faster
    fast_mode=True,                 # Skip interaction points
    enable_spatial_tracking=True
)

# For BEST quality (slower):
ObjectTracker(
    max_parallel_requests=2,        # Sequential = most stable
    crop_target_size=512,           # Full resolution
    fast_mode=False,                # Full analysis
    enable_spatial_tracking=True
)
```

---

## Expected Performance After Fixes

### Current: 31-35 seconds (8 objects)
### After Quick Fixes (1-3): 18-22 seconds (40% improvement)
### After Batching (4): 8-12 seconds (70% improvement)

---

## Monitoring Commands

```bash
# Watch detection timing in logs
tail -f outputs/tamp_demo/run_*/run_*.log | grep "TIMING"

# Check VLM request count
grep "generate_content" outputs/tamp_demo/run_*/run_*.log | wc -l

# View timing CSV
cat outputs/tamp_demo/run_*/run_*_timing.log
```
